import sys
import os
import json
import time
import re

# Add project root
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dashboard.utils.comparator import compare_documents

def test_real_files():
    base_dir = r"C:\Users\lilac\Desktop\æµ‹è¯•\1"
    tender_file = os.path.join(base_dir, "272ã€æ™‹èƒ½æŽ§è‚¡è£…å¤‡åˆ¶é€ é›†å›¢æœ‰é™å…¬å¸é‡‡ä¾›åˆ†å…¬å¸ç…¤çŸ¿ä¿¡æ¯åŒ–è®¾å¤‡åœ¨çº¿ç›‘æµ‹ç³»ç»Ÿ1(3).pdf")
    bid_a = os.path.join(base_dir, "å±±è¥¿å¯æ™ºå“è¯†æ ‡ä¹¦.pdf")
    bid_b = os.path.join(base_dir, "éƒžè…¾æ ‡ä¹¦1.pdf")
    
    print("Starting real file verification...")
    print(f"Tender: {tender_file}")
    
    start_time = time.time()
    try:
        result = compare_documents(bid_a, bid_b, tender_file)
        duration = time.time() - start_time
        
        print(f"Comparison finished in {duration:.2f} seconds.")
        
        collisions = result['paragraphs']
        print(f"Found {len(collisions)} collisions.")
        
        # User provided raw strings
        raw_targets = [
            "æˆ‘æ–¹æŠ•æ ‡æ–‡ä»¶çš„æœ‰æ•ˆæœŸå’Œæ‹›æ ‡æ–‡ä»¶è§„å®šçš„æŠ•æ ‡æœ‰æ•ˆæœŸä¸€è‡´ï¼Œæˆ‘æ–¹æ‰¿è¯ºåœ¨æ‹›æ ‡æ–‡ä»¶è§„å®šçš„æŠ•æ ‡æœ‰æ•ˆæœŸå†…ä¸æ’¤é”€æŠ•æ ‡èšŠä»¶",
            "13934518882",
            "SQL Server Always On"
        ]
        
        # Helper to normalize for checking
        def normalize_check(t):
             text = t.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼š', ':').replace('ï¼›', ';')
             text = text.replace('â€œ', '"').replace('â€', '"').replace("â€˜", "'").replace("â€™", "'")
             return re.sub(r'\s+', '', text)
        
        # Define targets with expected segments if needed
        # We just need to know if coverage is "good enough" or "found something"
        raw_targets = [
            "æˆ‘æ–¹æŠ•æ ‡æ–‡ä»¶çš„æœ‰æ•ˆæœŸå’Œæ‹›æ ‡æ–‡ä»¶è§„å®šçš„æŠ•æ ‡æœ‰æ•ˆæœŸä¸€è‡´ï¼Œæˆ‘æ–¹æ‰¿è¯ºåœ¨æ‹›æ ‡æ–‡ä»¶è§„å®šçš„æŠ•æ ‡æœ‰æ•ˆæœŸå†…ä¸æ’¤é”€æŠ•æ ‡èšŠä»¶",
            "13934518882",
            "SQL Server Always On",
            "å‡¡æˆ‘å…¬å¸å”®å‡ºçš„äº§å“ï¼Œä¿ä¿®æœŸé—´ä¸€åˆ‡å› äº§å“è´¨é‡è€Œå¼•èµ·çš„äº§å“æ•…éšœåŠæŸåï¼Œæœ¬ä¸­å¿ƒå‡å°†æä¾›å…è´¹ä¸Šé—¨ç»´ä¿®åŠæ›´æ¢é›¶é…ä»¶æœåŠ¡"
        ]

        target_map = {}
        for t in raw_targets:
            # key: raw target
            # value: list of normalized segments (split by comma)
            norm = normalize_check(t)
            segs = re.split(r'[,]', norm)
            target_map[t] = [s for s in segs if len(s) > 5]

        found_map = {t: set() for t in raw_targets} # Track which segments found
        
        # Print first 20 collisions
        print("-" * 30)
        for i, c in enumerate(collisions):
            text = c['text_a'] # This is normalized content
            if i < 10: 
                print(f"[{i}] [{c['type']}] {text[:50]}... (Page {c['page_a']})")
            
            for raw, segs in target_map.items():
                for seg in segs:
                    if seg in text:
                        found_map[raw].add(seg)
        
        print("\nVerification Results:")
        for raw, found_segs in found_map.items():
            total_segs = len(target_map[raw])
            found_count = len(found_segs)
            status = "âœ… FOUND" if found_count > 0 else "âŒ MISSING"
            print(f"  Target: {raw[:20]}... [{found_count}/{total_segs} segments match]")
            if found_count > 0 and found_count < total_segs:
                print(f"     (Partial match. Found: {list(found_segs)})")
        
        # Explicit check for typo keyword in ALL collisions
        typo_found = False
        for c in collisions:
             if "èšŠä»¶" in c['text_a']:
                 print(f"  ðŸ” TYPO CONFIRMED in collision: {c['text_a']}")
                 typo_found = True
                 break
        if not typo_found:
             print("  âš ï¸ TYPO 'èšŠä»¶' NOT FOUND in any collision.")

            
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_real_files()

